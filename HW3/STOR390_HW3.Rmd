---
title: "HW 3"
author: "Tianrui Ye"
date: "11/27/2023"
output: 
  html_document:
    number_sections: true
---
# 
In this homework, we will discuss support vector machines and tree-based methods.  I will begin by simulating some data for you to use with SVM. 
```{r}
library(e1071)
set.seed(1) 
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x, col=y)
```

##
Quite clearly, the above data is not linearly separable.  Create a training-testing partition with 100 random observations in the training partition.  Fit an svm on this training data using the radial kernel, and tuning parameters $\gamma=1$, cost $=1$.  Plot the svm on the training data.  
```{r}
set.seed(123)
train_indices <- sample(1:nrow(dat), 100)
train <- dat[train_indices, ]
test <- dat[-train_indices, ]
svm_model <- svm(y ~ ., data = train, method = "C-classification", kernel = "radial", gamma = 1, cost = 1)
plot(svm_model, train)

```
##
Notice that the above decision boundary is decidedly non-linear.  It seems to perform reasonably well, but there are indeed some misclassifications.  Let's see if increasing the cost ^[Remember this is a parameter that decides how smooth your decision boundary should be] helps our classification error rate.  Refit the svm with the radial kernel, $\gamma=1$, and a cost of 10000.  Plot this svm on the training data. 
```{r}
svm_model_high_cost <- svm(y ~ ., data = train, method = "C-classification", kernel = "radial", gamma = 1, cost = 10000)
plot(svm_model_high_cost, train)

```
##
It would appear that we are better capturing the training data, but comment on the dangers (if any exist), of such a model. 
*Student Answer*

Increasing the cost parameter significantly makes the SVM more flexible, allowing it to better capture the training data. However, this comes with the risk of overfitting. Overfitting happens when the model learns the training data too well, including its noise and outliers, which can lead to poor generalization on unseen data. Essentially, while the model may perform excellently on the training data, its performance on new, unseen data (such as the test set) may degrade.

##
Create a confusion matrix by using this svm to predict on the current testing partition.  Comment on the confusion matrix.  Is there any disparity in our classification results?    
```{r}
table(true=dat[-train_indices,"y"], pred=predict(svm_model_high_cost, newdata=dat[-train_indices,]))
```




There is a disparity in the classification results, mainly seen in the higher number of false negatives for class 1 compared to the false positives for class 2. This suggests that while the model is cautious not to wrongly classify instances as class 2 (likely due to the higher cost of misclassifications set during training), it is still prone to misclassifying class 1 instances as class 2.

##
Is this disparity because of imbalance in the training/testing partition?  Find the proportion of class `2` in your training partition and see if it is broadly representative of the underlying 25\% of class 2 in the data as a whole.  
```{r}
proportion_class_2_train <- sum(train$y == 2) / nrow(train)
proportion_class_2_total <- sum(dat$y == 2) / nrow(dat)

print(proportion_class_2_train)
print(proportion_class_2_total)
```
*Student Response*

The class 2 in the training data is higher than total. It might indicate that the training data is not representative of the overall data distribution. This imbalance can cause the model to perform better on the more represented class since it "sees" more examples of it during training, leading to a bias in predictions.

##
Let's try and balance the above to solutions via cross-validation.  Using the `tune` function, pass in the training data, and a list of the following cost and $\gamma$ values: {0.1, 1, 10, 100, 1000} and {0.5, 1,2,3,4}.  Save the output of this function in a variable called `tune.out`.  
```{r}
set.seed(1)
tune.out <- tune(svm, y ~ ., data=dat[train_indices,], 
                 kernel="radial", 
                 ranges=list(cost=c(0.1, 1, 10, 100, 1000), gamma=c(0.5, 1, 2, 3, 4)))

print(tune.out$best.parameters)
```
I will take `tune.out` and use the best model according to error rate to test on our data.  I will report a confusion matrix corresponding to the 100 predictions.  

```{r, eval = FALSE}
table(true=dat[-train_indices,"y"], pred=predict(tune.out$best.model, newdata=dat[-train_indices,]))
```
##
Comment on the confusion matrix.  How have we improved upon the model in question 2 and what qualifications are still necessary for this improved model.  
*Student Response*

The most notable improvement is the elimination of Type II errors for the second class (false negatives for class 2). The model has improved from misclassifying 2 instances of class 2 as class 1, to correctly classifying all instances of class 2. 

The improvement in identifying class 2 correctly should not overshadow the need for balance. The model should aim for a high recall without sacrificing precision unduly.

# 
Let's turn now to decision trees.  
```{r}
library(kmed)
data(heart)
library(tree)
```


## 
The response variable is currently a categorical variable with four levels.  Convert heart disease into binary categorical variable.  Then, ensure that it is properly stored as a factor. 
```{r}
heart$class <- ifelse(heart$class == 0, 0, 1)

heart$class <- as.factor(heart$class)
levels(heart$class) <- c("No Heart Disease", "Has Heart Disease")
```
## 
Train a classification tree on a 240 observation training subset (using the seed I have set for you).  Plot the tree.  
```{r}
set.seed(101)
train_indices <- sample(1:nrow(heart), 240)
train_data <- heart[train_indices, ]

heart_tree <- tree(class ~ ., data = train_data)

# 绘制树
plot(heart_tree)
text(heart_tree, pretty = 1)
```

## 
Use the trained model to classify the remaining testing points.  Create a confusion matrix to evaluate performance.  Report the classification error rate.  
```{r}
test_data <- heart[-train_indices, ]

predictions <- predict(heart_tree, test_data, type = "class")

conf_matrix <- table(Predicted = predictions, Actual = test_data$class)

print(conf_matrix)

error_rate <- sum(predictions != test_data$class) / nrow(test_data)

print(paste("Classification error rate:", error_rate))

```
##  
Above we have a fully grown (bushy) tree.  Now, cross validate it using the `cv.tree` command.  Specify cross validation to be done according to the misclassification rate.  Choose an ideal number of splits, and plot this tree.  Finally, use this pruned tree to test on the testing set.  Report a confusion matrix and the misclassification rate.  
```{r}
set.seed(101)
cv_heart_tree <- cv.tree(heart_tree, FUN=prune.misclass)

optimal_size <- cv_heart_tree$size[which.min(cv_heart_tree$dev)]

pruned_tree <- prune.misclass(heart_tree, best = optimal_size)
plot(pruned_tree)
text(pruned_tree, pretty = 1)

pruned_predictions <- predict(pruned_tree, test_data, type = "class")

pruned_conf_matrix <- table(Predicted = pruned_predictions, Actual = test_data$class)
print(pruned_conf_matrix)

pruned_error_rate <- sum(pruned_predictions != test_data$class) / nrow(test_data)
print(paste("Misclassification rate:", pruned_error_rate))
```

##
Discuss the trade-off in accuracy and interpretability in pruning the above tree. 
*Student Input *

Pruning a decision tree is a process of finding a balance between model accuracy and interpretability. A fully grown decision tree (i.e., an unpruned tree) tends to be very complex, containing many branches and leaf nodes, which allows it to achieve high accuracy on training data. However, this high complexity also makes the tree model susceptible to overfitting, i.e., the model is overly sensitive to noise or outliers in the training data, resulting in a decrease in its ability to generalize over new, unseen data.

By pruning, we can simplify the model by removing branches in the tree that do not contribute much to the final prediction. This has the direct result of improving the interpretability of the model, as a smaller tree is easier to understand and interpret. Also, while pruning may sacrifice some of the accuracy on the training data, it improves the model's ability to generalize over unknown data by reducing the complexity of the model, which is a worthwhile trade-off in many cases.


## 
Discuss the ways a decision tree could manifest algorithmic bias.  
*Student Answer*

Data imbalance: if the training data is imbalanced in some categories or features, the decision tree may be biased towards more diverse categories or features. This may result in poor predictive performance of the model for a few categories or features.

Feature Selection Bias: Decision trees are usually based on metrics such as information gain or Gini impurity when selecting features for splitting. If these metrics are misleading due to some bias in the data, the decision tree may prioritize features that are not actually the most discriminating.
